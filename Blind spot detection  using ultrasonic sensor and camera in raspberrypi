import RPi.GPIO as GPIO
import time
import cv2
import numpy as np
GPIO.setmode(GPIO.BOARD)
GPIO.setwarnings(False)


def distance():
    GPIO.output(TRIG, False)
    time.sleep(2)
    
    GPIO.output(TRIG, True)
    time.sleep(0.00001)
    GPIO.output(TRIG, False)
    
    while GPIO.input(ECHO) == 0:
        pulse_start = time.time()
    
    while GPIO.input(ECHO) == 1:
        pulse_end = time.time()
    
    pulse_duration = pulse_end - pulse_start
    distance = pulse_duration * 17150
    distance = round(distance, 2)
    print ("distance:",distance,"cm")
    return distance


# Specify the paths to the configuration and weights files
cfg_path = "/home/pi/yolo/yolov3-tiny.cfg"
weights_path = "/home/pi/yolo/yolov3-tiny.weights"
names_path = "/home/pi/yolo/coco.names"

# Load YOLO model
net = cv2.dnn.readNet(weights_path, cfg_path)

# Load COCO class labels
with open(names_path, "r") as f:
    classes = [line.strip() for line in f.readlines()]

# Get the output layer names
layer_names = net.getLayerNames()
output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]

TRIG=23
ECHO= 24

GPIO.setup(TRIG,GPIO.OUT)
GPIO.setup(ECHO,GPIO.IN)

GPIO.output(TRIG, False)
print ("Calibrating.......")
time.sleep(0.05)

while True:
    dist = distance()
    if dist<50:
        cap = cv2.VideoCapture(0)

        while True:
            # Capture frame-by-frame
            ret, frame = cap.read()
            height, width, channels = frame.shape

            # Prepare the frame for YOLO
            blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)
            net.setInput(blob)
            outs = net.forward(output_layers)

            # Initialize lists for detected bounding boxes, confidences, and class IDs
            class_ids = []
            confidences = []
            boxes = []

            # Analyze the outputs
            for out in outs:
                for detection in out:
                    scores = detection[5:]
                    class_id = np.argmax(scores)
                    confidence = scores[class_id]

                    # Filter out weak detections
                    if confidence > 0.3 and classes[class_id] in ["car", "bus", "motorbike", "truck", "bicycle"]:
                        center_x = int(detection[0] * width)
                        center_y = int(detection[1] * height)
                        w = int(detection[2] * width)
                        h = int(detection[3] * height)

                        # Rectangle coordinates
                        x = int(center_x - w / 2)
                        y = int(center_y - h / 2)

                        boxes.append([x, y, w, h])
                        confidences.append(float(confidence))
                        class_ids.append(class_id)

            # Apply Non-Maximum Suppression to remove multiple boxes for the same object
            indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.3, 0.4)

            if len(indexes) > 0:
                for i in indexes.flatten():
                    x, y, w, h = boxes[i]
                    label = str(classes[class_ids[i]])
                    confidence = confidences[i]
                    color = (0, 255, 0)

                    # Draw the bounding box and label on the frame
                    cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)
                    cv2.putText(frame, f"{label}: {confidence:.2f}", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

            # Display the output frame
            cv2.imshow("Vehicle Detection", frame)
            d = int(distance())
            # Break the loop if 'q' is pressed
            if cv2.waitKey(1) & d>50:
                break

        # Release the video stream and close windows
        cap.release()
        cv2.destroyAllWindows()
